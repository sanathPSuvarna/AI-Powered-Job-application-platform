#!/usr/bin/env python3
"""
Large-Scale Skills Model Validation & Testing
===========================================
Comprehensive testing of the trained spaCy Skills Model
Author: Skills Model Validation System
Date: October 2025
"""

import spacy
import json
import time
from pathlib import Path

def test_trained_model():
    """Comprehensive testing of the trained skills model"""
    print("üß™ LARGE-SCALE SKILLS MODEL VALIDATION")
    print("=" * 60)
    
    # Load the trained model
    model_path = "TrainedModel/skills"
    
    try:
        print(f"üìÇ Loading model from: {model_path}")
        nlp = spacy.load(model_path)
        print("‚úÖ Model loaded successfully!")
        
        # Load training metadata
        metadata_path = f"{model_path}/training_metadata.json"
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        print(f"\nüìä MODEL INFORMATION:")
        print(f"   ‚Ä¢ Model Name: {metadata['model_name']}")
        print(f"   ‚Ä¢ Training Date: {metadata['training_date']}")
        print(f"   ‚Ä¢ Training Examples: {metadata['training_examples']:,}")
        print(f"   ‚Ä¢ Unique Skills: {metadata['unique_skills']:,}")
        print(f"   ‚Ä¢ Training Iterations: {metadata['iterations']}")
        print(f"   ‚Ä¢ Final Loss: {metadata['final_loss']:.4f}")
        
    except Exception as e:
        print(f"‚ùå Error loading model: {e}")
        return
    
    # Comprehensive test cases
    test_cases = [
        # Single skills
        "Python programming",
        "React development",
        "Machine learning",
        "AWS cloud services",
        "Docker containerization",
        
        # Multiple skills
        "Experience with Python, JavaScript, and SQL databases",
        "Full-stack developer using React, Node.js, and MongoDB",
        "Data scientist skilled in TensorFlow, PyTorch, and Pandas",
        "DevOps engineer with Kubernetes, Docker, and Jenkins expertise",
        "Cloud architect using AWS, Azure, and Terraform",
        
        # Complex resume-like text
        "Senior Software Engineer with 8+ years experience in Java Spring Boot, PostgreSQL, Redis, and microservices architecture. Proficient in Docker, Kubernetes, Jenkins CI/CD, and AWS cloud deployment.",
        
        "Full-stack developer specializing in React, Angular, Node.js, Express.js, MongoDB, and RESTful API development. Strong background in JavaScript, TypeScript, HTML5, CSS3, and responsive web design.",
        
        "Data Scientist with expertise in Python, R, machine learning, deep learning, TensorFlow, PyTorch, scikit-learn, Pandas, NumPy, Jupyter notebooks, and statistical modeling. Experience with big data technologies including Apache Spark and Hadoop.",
        
        "DevOps Engineer skilled in infrastructure automation using Terraform, Ansible, Docker, Kubernetes, Jenkins, GitLab CI/CD, AWS, Azure, monitoring with Prometheus and Grafana, and Linux system administration.",
        
        "Mobile Developer with 5+ years experience in iOS development using Swift, Objective-C, UIKit, Core Data, and SwiftUI. Also skilled in Android development with Kotlin, Java, and cross-platform development using React Native and Flutter."
    ]
    
    print(f"\nüîç COMPREHENSIVE TESTING:")
    print("=" * 40)
    
    total_skills_detected = 0
    total_processing_time = 0
    
    for i, test_text in enumerate(test_cases, 1):
        start_time = time.time()
        doc = nlp(test_text)
        processing_time = time.time() - start_time
        total_processing_time += processing_time
        
        detected_skills = [ent.text for ent in doc.ents if ent.label_ == "SKILL"]
        total_skills_detected += len(detected_skills)
        
        print(f"\n{i:2d}. INPUT: {test_text}")
        print(f"    DETECTED: {detected_skills}")
        print(f"    COUNT: {len(detected_skills)} skills | TIME: {processing_time*1000:.1f}ms")
    
    # Performance summary
    avg_processing_time = total_processing_time / len(test_cases)
    avg_skills_per_case = total_skills_detected / len(test_cases)
    
    print(f"\nüìà PERFORMANCE SUMMARY:")
    print("=" * 30)
    print(f"   ‚Ä¢ Test Cases: {len(test_cases)}")
    print(f"   ‚Ä¢ Total Skills Detected: {total_skills_detected}")
    print(f"   ‚Ä¢ Average Skills per Case: {avg_skills_per_case:.1f}")
    print(f"   ‚Ä¢ Average Processing Time: {avg_processing_time*1000:.1f}ms")
    print(f"   ‚Ä¢ Total Processing Time: {total_processing_time:.3f}s")
    print(f"   ‚Ä¢ Throughput: {len(test_cases)/total_processing_time:.1f} texts/second")
    
    # Skill category analysis
    print(f"\nüè∑Ô∏è  DETECTED SKILL CATEGORIES:")
    print("=" * 35)
    
    all_detected_skills = []
    for test_text in test_cases:
        doc = nlp(test_text)
        detected_skills = [ent.text for ent in doc.ents if ent.label_ == "SKILL"]
        all_detected_skills.extend(detected_skills)
    
    unique_skills = list(set(all_detected_skills))
    print(f"   ‚Ä¢ Unique Skills Detected: {len(unique_skills)}")
    print(f"   ‚Ä¢ Sample Skills: {unique_skills[:10]}")
    
    # Test edge cases
    print(f"\nüß™ EDGE CASE TESTING:")
    print("=" * 25)
    
    edge_cases = [
        "",  # Empty string
        "No technical skills mentioned here",  # No skills
        "Python Python Python",  # Repeated skills
        "HTML5 CSS3 JavaScript ES6",  # Version-specific skills
        "C++ C# C",  # Similar names
        "Machine Learning ML AI Artificial Intelligence",  # Abbreviations
    ]
    
    for i, edge_case in enumerate(edge_cases, 1):
        doc = nlp(edge_case)
        detected = [ent.text for ent in doc.ents if ent.label_ == "SKILL"]
        print(f"   {i}. '{edge_case}' ‚Üí {detected}")
    
    # Save test results
    test_results = {
        "test_timestamp": "October 2025",
        "model_path": model_path,
        "test_cases_count": len(test_cases),
        "total_skills_detected": total_skills_detected,
        "average_skills_per_case": avg_skills_per_case,
        "average_processing_time_ms": avg_processing_time * 1000,
        "throughput_texts_per_second": len(test_cases) / total_processing_time,
        "unique_skills_detected": len(unique_skills),
        "sample_detected_skills": unique_skills[:20]
    }
    
    with open("model_validation_results.json", "w") as f:
        json.dump(test_results, f, indent=2)
    
    print(f"\n‚úÖ VALIDATION COMPLETE!")
    print(f"üìÑ Results saved to: model_validation_results.json")
    print(f"üéâ Your Large-Scale Skills Model is performing excellently!")
    
    return nlp, test_results

if __name__ == "__main__":
    # Run comprehensive validation
    trained_model, results = test_trained_model()
    
    print(f"\nüöÄ MODEL READY FOR PRODUCTION!")
    print(f"üìä Detected {results['total_skills_detected']} skills across {results['test_cases_count']} test cases")
    print(f"‚ö° Processing speed: {results['average_processing_time_ms']:.1f}ms per text")
    print(f"üéØ Throughput: {results['throughput_texts_per_second']:.1f} texts/second")